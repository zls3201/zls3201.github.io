{
	"data" : 
	{"Content":"\n\n\u003ch3 id=\"线性回归\"\u003e线性回归\u003c/h3\u003e\n\n\u003ch2 id=\"1-线性回归数学\"\u003e1.线性回归数学\u003c/h2\u003e\n\n\u003cp\u003e\n输入{x,y},参数$\\theta$,假设函数为$H(\\theta)$,损失函数为$J(\\theta)$\n目标:根于样本输入，确定$\\theta$,是损失函数$J(\\theta)$尽可能小\n\u003c/p\u003e\n\n\u003cp\u003e\n$$Input : X ,Y \\\nX\\text{ is }\n\\begin{bmatrix}\n   x^{(1)} \\\\\n   x^{(2)} \\\\\n   ... \\\\\n   x^{(m-1)} \\\\\n   x^{(m)}\n\\end{bmatrix}   \\ Y  \\text{ is }\n\\begin{bmatrix}\n   y_1 \\\\\n   y_2 \\\\\n   ... \\\\\n   y_{m-1} \\\\\n   y_m\n\\end{bmatrix}  \\\nParameter : \\theta \\text{ is }\n\\begin{bmatrix}\n   \\theta_0 \\\\\n   \\theta_1 \\\\\n   \\theta_2 \\\\\n   ... \\\\\n   \\theta_{n-1} \\\\\n   \\theta_n\n\\end{bmatrix} $$\n\u003c/p\u003e\n\n\u003cp\u003e\n$$X\\text{是一个矩阵，m行（m为样本数量），n+1列。 } x^{(i)}  \\text{ 是一个向量  }   [x_0,x_1,x_2,...,x_n] \\text{ 其中}x_0\\text{为1}$$\n\u003c/p\u003e\n\n\u003cp\u003e\n$$\nY\\text{是一个列向量，m行， }Y_i \\text{ 是一个数值， }\n$$\n\u003c/p\u003e\n\n\u003cp\u003e\n$$\\theta\\text{是一个列向量，行数为n+1。}\n\\theta_i \\text{ 是一个数值 }$$\n\u003c/p\u003e\n\n\u003cp\u003e\n$$\\text{Hypothesis is :} \\ h_{\\theta}\\left( x \\right) =  \\theta^T X$$\n\u003c/p\u003e\n\n\u003cp\u003e\n$$\n\\begin{aligned}\n    \\text{ Lost Function: }\n\\end{aligned}\n\n\\begin{aligned}\nJ(\\theta)=\\frac 1 m  \\sum^{m}_{i=1} \\frac 1 2 (h_{\\theta}(x^{(i)})-y^{(i)})^2\n\\end{aligned}\n$$\n\u003c/p\u003e\n\n\u003cp\u003e\n为了获取$J(\\theta)$的最小值，利用高斯下降算法\n$$\n\\boxed{\\begin{aligned}\n\\theta_j\u0026:=\\theta_{j}-\\alpha\\frac{\\partial}{\\partial\\theta_j}J(\\theta) \\\\\n\u0026:=\\theta_j- \\alpha\\cdot\\frac{1}{2}\\frac{\\partial}{\\partial\\theta_j}(h_{\\theta}(x)-y)^2 \\\\\n\u0026:=\\theta_j- \\alpha\\cdot2\\cdot\\frac{1}{2}(h_{\\theta}(x)-y)(\\frac{\\partial}{\\partial\\theta_j}(h_{\\theta}(x)-y)) \\\\\n\u0026:=\\theta_j- \\alpha\\cdot(h(\\theta)-y)\\cdot\\frac{\\partial}{\\partial\\theta_j}(\\sum^{m}_{i=0}\\theta_ix_i-y_i) \\\\\n\u0026:=\\theta_j- \\alpha\\cdot(h_{\\theta}(x)-y)x_j\n\\end{aligned}}\n$$\n\u003c/p\u003e\n\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# -*- coding: utf-8 -*-\n\u0026quot;\u0026quot;\u0026quot;\nCreated on Sat Jun  9 08:18:10 2018\n\n@author: zls3201@gmail.com\n\u0026quot;\u0026quot;\u0026quot;\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib import animation as amat\n\ndef loadDataSet(fileName):\n    \u0026quot;\u0026quot;\u0026quot;从文本文件加载数据.\n\n    文件内容格式.\n\n    Args:\n        path: 传入文件名称.\n\n    Returns:\n        返回一个参数矩阵x和一个结论列向量y. For\n        example:\n\n        {'Serak': ('Rigel VII', 'Preparer'),\n         'Zim': ('Irk', 'Invader'),\n         'Lrrr': ('Omicron Persei 8', 'Emperor')}\n\n        If a key from the keys argument is missing from the dictionary,\n        then that row was not found in the table.\n\n    Raises:\n        IOError: An error occurred accessing the bigtable.Table object.\n    \n    \u0026quot;\u0026quot;\u0026quot;\n    dataX=[];dataY=[]\n    fr=open(fileName)\n    for line in fr.readlines():\n        dataArr= []\n        for item in  line.strip().split('\\t'):\n            dataArr.append(float(item))\n       \n        dataX.append( dataArr[0:-1])\n        dataY.append( dataArr[-1])\n    \n    print(dataX)\n    print(dataY)\n    return np.array( dataX),np.array(dataY)\n\ndef dataNomalize(dataX,dataY):\n    \u0026quot;\u0026quot;\u0026quot;对数据进行归一化处理.\n\n    文件内容格式.\n\n    Args:\n        matriX: 参数组成的矩阵x，每行为一组样本个例参数.\n        vertY: 结论值组成的向量y.\n\n    Returns:\n        返回归一化后的一个参数矩阵x和一个结论列向量y. For\n        example:\n\n        {'Serak': ('Rigel VII', 'Preparer'),\n         'Zim': ('Irk', 'Invader'),\n         'Lrrr': ('Omicron Persei 8', 'Emperor')}\n\n        If a key from the keys argument is missing from the dictionary,\n        then that row was not found in the table.\n\n    Raises:\n        IOError: An error occurred accessing the bigtable.Table object.\n    \n    \u0026quot;\u0026quot;\u0026quot;\n    m,n=np.shape(dataX)\n    colMaxMin=[dataX.max(axis=0),dataX.min(axis=0)]\n\n    for i in range(m):\n        for j in range(n):\n            #print (i,j,dataX[i][j])\n            dataX[i][j]= (dataX[i][j]-colMaxMin[1][j])/colMaxMin[0][j]\n\n    minY=dataY.min();maxY=dataY.max()\n\n    for i in range(len(dataY)):\n        dataY[i]=(dataY[i]-minY)/maxY\n\n    print(dataX)\n    print(dataY)\n    return dataX,dataY\n\ndef computeCost(x,y,theta):\n    m = y.shape[0]\n#     J = (np.sum((X.dot(theta) - y)**2)) / (2*m)\n    C = x.dot(theta) - y\n    J2 = (C.T.dot(C))/ (2*m)\n    return J2\n\ndef batchGradientDescent(dataX,dataY):\n    matX=np.mat(dataX)\n    matY=np.mat(dataY).transpose()\n    m,n=np.shape(matX)\n    alpha=0.001\n    theta=np.ones((n,1))\n    maxTimes=500\n    for k in range(maxTimes):\n        theta=theta-(alpha/m)*(np.dot(matX.T,np.dot(matX,theta)-matY))\n        cost=computeCost(matX,matY,theta)\n        print(theta,cost)\n    return theta\n\ndef stochastieGradientDeccent():\n    return 3\n\ndef miniBatchGradientDescent():\n    return 4\n\nif __name__ == '__main__':\n    dataX,dataY=loadDataSet('house_prize.txt')\n    dataX,dataY=dataNomalize(dataX,dataY)\n    batchGradientDescent(dataX,dataY)\n\n\n\u003c/code\u003e\u003c/pre\u003e\n","date":"2018-07-12T08:00:35+08:00","permalink":"https://zls3201.github.io/post/linear_regression/","summary":"线性回归 1.线性回归数学 输入{x,y},参数$\\theta$,假设函数为$H(\\theta)$,损失函数为$J(\\theta)$ 目标:根于样","title":"线性回归","type":"post"}

}