{
	"data" : 
	{"Content":"\u003ch1 id=\"ml\"\u003eML\u003c/h1\u003e\n\u003ch2 id=\"资料整理\"\u003e资料整理\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://zh.coursera.org/learn/machine-learning\"\u003e机器学习课程\u003c/a\u003e\n\u003ca href=\"http://open.163.com/special/opencourse/machinelearning.html\"\u003eCS229 网易公开课_吴恩达_机器学习_2008\u003c/a\u003e\n\u003ca href=\"https://github.com/Kivy-CN/Stanford-CS-229-CN\"\u003eCS229 课程讲义中文翻译\u003c/a\u003e\n\u003ca href=\"https://www.gitbook.com/book/yoyoyohamapi/mit-ml\"\u003e斯坦福机器学习笔记\u003c/a\u003e\n\u003ca href=\"https://www.julyedu.com/course/getDetail/110\"\u003e机器学习第九期\u003c/a\u003e\n\u003ca href=\"https://legacy.gitbook.com/book/wizardforcel/dm-algo-top10/details\"\u003e数据挖掘十大算法详解\u003c/a\u003e\n\u003ca href=\"https://www.coursera.org/learn/ml-foundations\"\u003e机器学习基础：案例研究\u003c/a\u003e\n\u003ca href=\"http://www.fast.ai/\"\u003e神经网络_fast.ai\u003c/a\u003e\n\u003ca href=\"\"\u003e西瓜书\u003c/a\u003e\n\u003ca href=\"\"\u003e南瓜书\u003c/a\u003e\n机器学习实践\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.csie.ntu.edu.tw/~htlin/\"\u003ehttps://www.csie.ntu.edu.tw/~htlin/\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"1常见问题\"\u003e1.常见问题\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e监督学习与非监督学习\u003c/li\u003e\n\u003cli\u003e参数学习与非参数学习\u003c/li\u003e\n\u003cli\u003e判定模型与生成模型\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"chroma\"\u003e\u003ccode class=\"language-text\" data-lang=\"text\"\u003e\nhttps://www.zhihu.com/question/20446337/answer/45130939\n\n \n\n我是这样理解的： \n\n生成模型，就是生成（数据的分布）的模型； \n\n判别模型，就是判别（数据输出量）的模型； \n\n更进一步： \n\n从结果角度，两种模型都能给你 输出量（label 或 y etc.)。 \n\n但，生成模型的处理过程会告诉你关于数据的一些统计信息（p(x|y) 分布 etc.），更接近于统计学； \n\n而 判别模型则是通过一系列处理得到结果，这个结果可能是概率的或不是，这个并不改变他是不是判别的。 \n\n如，决策树的if then说不是这个就是那个（而很多属性都是有分布的）【即分支】，明显是一种 判别 嘛； \n\n而朴素贝叶斯说，p( cancer , fat ) = x% etc.，模型 生成 了一个分布给你了，即使你没意识到/没用到，只用到 p( cancer | fat ) = y% 这个最终的判别。 \n\n你再理解一下： \n\nk近邻法、感知机、逻辑斯谛回归模型、最大熵模型、支持向量机、提升方法是判别模型； \n\n隐马尔可夫模型（重点的EM算法）是生成模型。 \n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cpre\u003e\u003ccode class=\"language-mermaid\" data-lang=\"mermaid\"\u003egraph LR\n    A[Hard edge] --\u0026gt;|Link textQQ  | B(Round edge)\n    B --\u0026gt; C{Decision}\n    C --\u0026gt;|One| D[Result one]\n    C --\u0026gt;|Two| E[Result two]\n\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"2常用算法\"\u003e2.常用算法\u003c/h2\u003e\n\u003ch3 id=\"21-线性回归\"\u003e2.1 线性回归\u003c/h3\u003e\n\u003ch4 id=\"211-局部加权线性回归\"\u003e2.1.1 局部加权线性回归\u003c/h4\u003e\n\u003cpre\u003e\u003ccode\u003ehttps://blog.csdn.net/Allenalex/article/details/16370245 \nhttps://blog.csdn.net/tianse12/article/details/70161591 \nhttps://blog.csdn.net/herosofearth/article/details/51969517 \n\u003c/code\u003e\u003c/pre\u003e\u003ch4 id=\"212-广义线性模型\"\u003e2.1.2 广义线性模型\u003c/h4\u003e\n\u003ch3 id=\"22-逻辑回归\"\u003e2.2 逻辑回归\u003c/h3\u003e\n\u003ch4 id=\"221-softmax回归\"\u003e2.2.1 SoftMax回归\u003c/h4\u003e\n\u003ch3 id=\"23-感知器算法\"\u003e2.3 感知器算法\u003c/h3\u003e\n\u003ch3 id=\"生成学习算法\"\u003e生成学习算法\u003c/h3\u003e\n\u003ch4 id=\"高斯判别分析\"\u003e高斯判别分析\u003c/h4\u003e\n\u003ch4 id=\"朴素贝叶斯\"\u003e朴素贝叶斯\u003c/h4\u003e\n\u003ch5 id=\"垃圾邮件分类\"\u003e垃圾邮件分类\u003c/h5\u003e\n\u003ch5 id=\"laplcae平滑\"\u003eLaplcae平滑\u003c/h5\u003e\n\u003ch3 id=\"svm支持向量机\"\u003eSVM（支持向量机）\u003c/h3\u003e\n\u003ch3 id=\"knnk近邻算法\"\u003eKNN(K近邻算法)\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://blog.csdn.net/suipingsp/article/details/41964713\"\u003ehttps://blog.csdn.net/suipingsp/article/details/41964713\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.cnblogs.com/ybjourney/p/4702562.html\"\u003ehttps://www.cnblogs.com/ybjourney/p/4702562.html\u003c/a\u003e\u003c/p\u003e\n\u003ch4 id=\"欧氏距离与曼哈顿距离\"\u003e欧氏距离与曼哈顿距离\u003c/h4\u003e\n\u003ch3 id=\"决策树\"\u003e决策树\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://zh.wikipedia.org/wiki/%E5%86%B3%E7%AD%96%E6%A0%91\"\u003ehttps://zh.wikipedia.org/wiki/%E5%86%B3%E7%AD%96%E6%A0%91\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://www.cnblogs.com/leoo2sk/archive/2010/09/19/decision-tree.html\"\u003ehttp://www.cnblogs.com/leoo2sk/archive/2010/09/19/decision-tree.html\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.cnblogs.com/pinard/p/6050306.html\"\u003ehttps://www.cnblogs.com/pinard/p/6050306.html\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"朴素贝叶斯分类\"\u003e朴素贝叶斯分类\u003c/h3\u003e\n\u003ch3 id=\"神经网络\"\u003e神经网络\u003c/h3\u003e\n\u003ch4 id=\"卷积神经网络\"\u003e卷积神经网络\u003c/h4\u003e\n\u003ch4 id=\"循环神经网络\"\u003e循环神经网络\u003c/h4\u003e\n\u003ch2 id=\"数学基础\"\u003e数学基础\u003c/h2\u003e\n\u003ch3 id=\"微积分\"\u003e微积分\u003c/h3\u003e\n\u003cp\u003e反函数求导\n矩阵微分\n牛顿法\n海森矩阵\n梯度下降\n批量梯度下降\n随即梯度下降\n自然对数e是如何发现的\u003c/p\u003e\n\u003ch3 id=\"概率论与数理统计\"\u003e概率论与数理统计\u003c/h3\u003e\n\u003cp\u003e概率分布\n指数族分布\n伯努力分布\n高斯分布（正态分布）\n多元正态分布\n伽马分布\u003c/p\u003e\n\u003cp\u003e概率质量函数与概率密度函数\u003c/p\u003e\n\u003cp\u003e似然估计\n极大似然估计\nJoin似然估计\n最小二乘法\u003c/p\u003e\n\u003cp\u003e贝叶斯\u003c/p\u003e\n\u003ch3 id=\"线性代数与场论\"\u003e线性代数与场论\u003c/h3\u003e\n\u003ch4 id=\"矩阵迹运算\"\u003e矩阵迹运算\u003c/h4\u003e\n\u003ch4 id=\"矩阵的秩与满秩矩阵\"\u003e矩阵的秩与满秩矩阵\u003c/h4\u003e\n\u003ch4 id=\"矩阵分解与奇异值分解\"\u003e矩阵分解与奇异值分解\u003c/h4\u003e\n\u003ch3 id=\"常用算法\"\u003e常用算法\u003c/h3\u003e\n\u003ch4 id=\"牛顿法\"\u003e牛顿法\u003c/h4\u003e\n\u003ch4 id=\"最小二乘法\"\u003e最小二乘法\u003c/h4\u003e\n\u003ch4 id=\"似然估计\"\u003e似然估计\u003c/h4\u003e\n\u003ch4 id=\"高斯下降\"\u003e高斯下降\u003c/h4\u003e\n","date":"2018-09-06T07:00:00+08:00","permalink":"https://zls3201.github.io/post/ml_note/","summary":"ML 资料整理 机器学习课程 CS229 网易公开课_吴恩达_机器学习_2008 CS229 课程讲义中文翻译 斯坦福机器学习笔记 机器学习第九期 数据挖掘十大算法详解 机器学习","title":"机器学习笔记","type":"post"}

}